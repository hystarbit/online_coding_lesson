ㄹㅇ쉬운 딥러닝 5강 : 
신나는 경사하강법
                           0.18
70   0.1   7   0.9    0.2  예측값
성적 --w1-- h1|sig(h1)--w2-- y^

Loss구하는법
총손실 E = (1/n) * Σ(y^ - y)^2
w최적화는 경사하강법으로
총손실 E를 최소화하는 w를 찾는 법?

Loss를 최소화하는 최적의 w값 찾기
     0.7                   예측값
성적 --w1-- h1|sig(h1)--w2-- y^

총손실 E   _
  |     / \   /
  |    /   \_- w1이 변할수록 최종 Loss값도 이리저리 바뀝니다.
  | \./     
  |__|_________ 
최적의 w는 이거   w1  최적의 w1값은 어디?

첫 w1값은 랜덤

총손실 E   _
  |     / \   /
  |    /   \_- 
  | \_/|     
  |____|________ 
       3      w1

변수가 w1 하나면 2차원 그래프 그려서 판단가능
변수가 수백개면..그림도 못그림
w1을 최적값으로 변경하려면 경사하강법을 씁니다.
= 현재 w1값에서의 접선의 기울기를 w1에서 빼셈
         .
  |     ./ \   /
  |    o   \_- 
  | \/.|     
  |__._|________ 
  0.5 23      w1

  
  |     / 
  |    /   기울기는 +2,+3 이정도
  |   /    
  |_____________ 

  |   \ 
  |    \   기울기는 +2,+3 이정도
  |     \    
  |_____________ 

           _
  |       / \   /
  |      /   \_- 
  | \ _ /     
  ||_|__________
  0.5|        w1
    1.5

새로운 w값 찾고싶으면요
새로운 w1 <= w1 - α(δE/δw1)
       기존의w1에서 기울기를 빼셈
w1이 총손실E에 얼마나 큰 영향을 미치는가(편미분..)
w1이 0.00001만큼 조금 변하면 E는 얼마나 크게 변하나
머신에게 러닝을 시킨다
=손실을 최소화하는 w값 찾게시킴
=그 방법은 경사하강임

딥러닝 학습과정
1. w값들 랜덤으로 찍음
2. w값 바탕으로 총손실 E를 계산함
3. 경사하강으로 새로운 w값 업데이트
새로운 w1 <= 기존 w1 - α(δE/δw1)
새로운 w2 <= 기존 w2 - α(δE/δw2)

새로운 w1
새로운 w2

4. w값 바탕으로 총손실 E를 계산함
5. 경사하강으로 새로운 w값 업데이트

6. w값 바탕으로 총손실 E를 계산함
7. 경사하강으로 새로운 w값 업데이트

...총손실 E가 더이상 안줄어들 때까지

w1 = 0.1 (랜덤)
w2 = 0.1 (랜덤)

for i in 여러번반복:
    (1) w1이랑 w2로 예측값 y 계산
    (2) 총손실 E 계산 (예측값 - 실제값 등)
    (3) w1 = 기존 w1 - ( w1이 E에 끼치는 영향 * learning rate )
    (4) w2 = 기존 w2 - ( w2이 E에 끼치는 영향 * learning rate )

local minima
새로운 w1 <= 기존 w1 - α(δE/δw1)
  |         _.-  
  |    /\_ /    \  /
  | \ /  △기울기가0 -  
  |___________________ 
                     w1 
경사하강 더이상 못함
이제 학습끝인데요

여길 찾아야되는데 ㄷㄷ

기울기만 빼지말고
learing rate * 기울기를 빼세요
α : learning rate
실제 최저점
가짜 최저점
learning rate
(일명 trial and rate)

고정된 값만 주면
복잡한 문제의 경우
학습이 안일어나는 경우도 있음


learning rate(α) optimizer
(학습 중간중간 learning rate를 어떤 식으로 수정할까)
- Momentum: 가속도를 유지하자
총손실 E   _
  |   ↓./ \   /
  |   ./   \_- 
  | \_/      
  |_____________ 
              w1
- AdaGrad: 자주변하는 w는 작게, 자주변하면 크게
- RMSProp: AdaGrad인데 제곱합
- AdaDelta: AdaGrad인데 a 너무 작아져서 학습 안되는거 방지해줌
- Adam: RMSProp + Momentum

여러개 써보고 모델과 어울리는거 택하시면 됩니다
보통 Adam

새로운 w1 <= 기존 w1 - α(δE/δw1)
w1이 쪼금 변하면
E는 얼마나 변하는가
(일명 편미분)

(노잼이라서 유튜브에선 뺐습니다
다음강의 스킵 ㄱㄱ)